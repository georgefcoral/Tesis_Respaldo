@comment{This file has been generated by Pybliographer}


@Book{Forsyth2003,
  Author         = {Forsyth, David A. and Ponce, Jean},
  Title          = {{c}omputer {V}ision: amodern approach},
  Publisher      = {Prentice Hall},
  Note           = {ISBN: 013-085198-1},
  year           = 2003
}

@article{Mo2012,
abstract = {In order to overcome the disadvantages of the traditional auto-focus algorithms which are poor in real-time performance, weak in anti-noise capability and vulnerable to the influence of contrast and background pixels, this paper proposes an auto-focus algorithm based on maximum gradient and threshold. It introduces a threshold factor and takes a new kind of adaptive threshold to remove the pixels contaminated by noise and background in the image, then uses improved Sobel operators to extract maximum gray gradient after image preprocessing and calculates evaluation value. The experimental results show that the proposed algorithm has good real-time performance, strong unimodality, high sensitivity and strong anti-noise capability. In addition, the algorithm is less influenced by contrast and background pixels of the image. It can also control the sensitivity and focusing range of the focusing function. So the proposed algorithm is most suitable for auto-focus subsystem of video monitoring and tracking system. {\textcopyright} 2012 IEEE.},
author = {Mo, Chunhong and Liu, Bo},
doi = {10.1109/CISP.2012.6469961},
file = {:C$\backslash$:/Users/IGNITER/Downloads/mo2012(1).pdf:pdf},
isbn = {9781467309622},
journal = {2012 5th Int. Congr. Image Signal Process. CISP 2012},
keywords = {Antinoise,Auto-focus,Gradient,Image Processing,Sensitivity,Threshold},
number = {Cisp},
pages = {1191--1194},
title = {{An auto-focus algorithm based on maximum gradient and threshold}},
year = {2012}
}

@article{Zhang:14,
abstract = {{\textcopyright} 2014 Optical Society of America. An auto-focus method for digital imaging systems is proposed that combines depth from focus (DFF) and improved depth from defocus (DFD). The traditional DFD method is improved to become more rapid, which achieves a fast initial focus. The defocus distance is first calculated by the improved DFD method. The result is then used as a search step in the searching stage of the DFF method. A dynamic focusing scheme is designed for the control software, which is able to eliminate environmental disturbances and other noises so that a fast and accurate focus can be achieved. An experiment is designed to verify the proposed focusing method and the results show that the method's efficiency is at least 3-5 times higher than that of the traditional DFF method.},
author = {Zhang, Xuedian and Liu, Zhaoqing and Jiang, Minshan and Chang, Min},
doi = {10.1364/oe.22.031237},
file = {:E$\backslash$:/Fast{\_}and{\_}accurate{\_}auto-focusing{\_}algorithm{\_}based{\_}on{\_}the{\_}combination{\_}of{\_}depth{\_}from{\_}focus{\_}and{\_}improved{\_}depth{\_}from{\_}defocus-Zhang{\_}et{\_}al-2014{\_}OE-Vol22{\_}Issue25{\_}PP31237-31247.pdf:pdf},
issn = {1094-4087},
journal = {Opt. Express},
mendeley-groups = {Depth of Field},
number = {25},
pages = {31237},
title = {{Fast and accurate auto-focusing algorithm based on the combination of depth from focus and improved depth from defocus}},
volume = {22},
year = {2014}
}

@article{Subbarao1993,
abstract = {We use the paraxial geometric optics model of image forma- tion to derive a set of camera focusing techniques. These techniques do not require calibration of cameras but involve a search of the camera parameter space. The techniques are proved to be theoretically sound under weak assumptions. They include energy maximization of unfil- tered, low-pass-filtered, high-pass-filtered, and bandpass-filtered im- ages. It is shown that in the presence of high spatial frequencies, noise, and aliasing, focusing techniques based on bandpass filters perform well. The focusing techniques are implemented on a prototype camera system called the Stonybrook passive autofocusing and ranging camera system (SPARCS). The architecture of SPARCS is described briefly. The performance of the different techniques are compared experimentally. All techniques are found to perform well. The energy of low-pass-filtered image gradient, which has better overall characteristics, is recommended for practical applications},
author = {Subbarao, Murali},
doi = {10.1117/12.147706},
file = {:E$\backslash$:/subbarao1993.pdf:pdf},
isbn = {0819410241},
issn = {00913286},
journal = {Opt. Eng.},
mendeley-groups = {Depth of Field},
number = {11},
pages = {2824},
title = {{Focusing techniques}},
volume = {32},
year = {1993}
}

@ARTICLE{Pentland,
  author={A. P. {Pentland}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A New Sense for Depth of Field}, 
  year={1987},
  volume={PAMI-9},
  number={4},
  pages={523-531},
  abstract={This paper examines a novel source of depth information: focal gradients resulting from the limited depth of field inherent in most optical systems. Previously, autofocus schemes have used depth of field to measured depth by searching for the lens setting that gives the best focus, repeating this search separately for each image point. This search is unnecessary, for there is a smooth gradient of focus as a function of depth. By measuring the amount of defocus, therefore, we can estimate depth simultaneously at all points, using only one or two images. It is proved that this source of information can be used to make reliable depth maps of useful accuracy with relatively minimal computation. Experiments with realistic imagery show that measurement of these optical gradients can provide depth information roughly comparable to stereo disparity or motion parallax, while avoiding image-to-image matching problems.},
  keywords={Focusing;Lenses;Biomedical optical imaging;Optical sensors;Humans;Brightness;Information resources;Motion measurement;Biological systems;Retina;Focus;human vision;image understanding;range sensing;shape-from-focus},
  doi={10.1109/TPAMI.1987.4767940},
  ISSN={1939-3539},
  month={July},}
  
@misc{encyclopediabritannica, title={Aperture}, url={https://www.britannica.com/technology/aperture-optics}, journal={Encyclopædia Britannica}, publisher={Encyclopædia Britannica, inc.}}


@misc{paschotta_2020, title={Diaphragms}, url={https://www.rp-photonics.com/diaphragms.html}, journal={RP Photonics Encyclopedia - diaphragms, holes, blades, photo camera}, publisher={RP Photonics}, author={Paschotta, Dr. Rüdiger}, year={2020}, month={Mar}} 

@misc{canon_australia_2019, title={Depth of Field}, url={https://www.canon.com.au/explore/glossary/depth-of-field}, journal={Canon Australia}, year={2019}, month={Mar}}


@book{book:{305007},
   title =     {Measuring Biological Responses with Automated Microscopy},
   author =    {James Inglese},
   publisher = {Academic press},
   isbn =      {9780121828196,0121828190},
   year =      {2006},
   series =    {Methods in Enzymology 414},
   edition =   {1},
   volume =    {},
   url =       {http://gen.lib.rus.ec/book/index.php?md5=115ac4f7dad557ecb8975991f0524863}}
   
    @misc{ruttenfusser_wilson_davidson, title={The Point Spread Function in Details}, url={https://www.zeiss.com/microscopy/int/solutions/reference/basic-microscopy/the-point-spread-function.html}, journal={The Point Spread Function}, publisher={Carl Zeiss Microscopy GmbH}, author={Ruttenfusser, Rudi and Wilson, Erin E. and Davidson, Michael W.}} 
    

    
@book{Flusser2009,
abstract = {Moments as projections of an image{\^{a}}"s intensity onto a proper polynomial basis can be applied to many different aspects of image processing. These include invariant pattern recognition, image normalization, image registration, focus/ defocus measurement, and watermarking. This book presents a survey of both recent and traditional image analysis and pattern recognition methods, based on image moments, and offers new concepts of invariants to linear filtering and implicit invariants. In addition to the theory, attention is paid to efficient algorithms for moment computation in a discrete domain, and to computational aspects of orthogonal moments. The authors also illustrate the theory through practical examples, demonstrating moment invariants in real applications across computer vision, remote sensing and medical imaging. Key features: Presents a systematic review of the basic definitions and properties of moments covering geometric moments and complex moments. Considers invariants to traditional transforms {\^{a}}" translation, rotation, scaling, and affine transform - from a new point of view, which offers new possibilities of designing optimal sets of invariants. Reviews and extends a recent field of invariants with respect to convolution/blurring. Introduces implicit moment invariants as a tool for recognizing elastically deformed objects. Compares various classes of orthogonal moments (Legendre, Zernike, Fourier-Mellin, Chebyshev, among others) and demonstrates their application to image reconstruction from moments. Offers comprehensive advice on the construction of various invariants illustrated with practical examples. Includes an accompanying website providing efficient numerical algorithms for moment computation and for constructing invariants of various kinds, with about 250 slides suitable for a graduate university course. Moments and Moment Invariants in Pattern Recognition is ideal for researchers and engineers involved in pattern recognition in medical imaging, remote sensing, robotics and computer vision. Post graduate students in image processing and pattern recognition will also find the book of interest. {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
author = {Flusser, Jan and Suk, Tom{\'{a}}{\v{s}} and Zitov{\'{a}}, Barbara},
booktitle = {Moments Moment Invariants Pattern Recognit.},
doi = {10.1002/9780470684757},
file = {:C$\backslash$:/Users/IGNITER/Documents/BOOKS/Jan Flusser, Barbara Zitova, Tomas Suk - Moments and Moment Invariants in Pattern Recognition-J. Wiley (2009).pdf:pdf},
isbn = {9780470699874},
pages = {1--296},
title = {{Moments and Moment Invariants in Pattern Recognition}},
year = {2009}
}

@book{Waltz2008,
author = {Waltz, E and Waltz, T and Liggins, M E and Hall, D L and Llinas, J},
file = {:C$\backslash$:/Users/IGNITER/Downloads/Handbook{\_}of{\_}Multisensor{\_}Data{\_}Fusion Teory{\_}and{\_}Practice-Hall{\_}et{\_}al-CRC{\_}Press.pdf:pdf},
isbn = {null},
pages = {89},
title = {{Handbook of multisensor data fusion}},
url = {https://xueshu.baidu.com/usercenter/paper/show?paperid=548485244e986944411bcfe08dceb23f{\&}site=xueshu{\_}se},
volume = {null},
year = {2008}
}


 @misc{cartesianDef, title={Cartesian coordinates}, url={https://mathworld.wolfram.com/CartesianCoordinates.html}, journal={from Wolfram MathWorld}} 
 
@misc{mallick_2021, title={Geometry of Image Formation | LearnOpenCV #}, url={https://learnopencv.com/geometry-of-image-formation/},journal={LearnOpenCV – OpenCV, PyTorch, Keras, Tensorflow examples and tutorials}, author={Mallick, Satya}, year={2021}}

 @book{hartley_zisserman_2004, place={Cambridge}, title={Multiple view geometry in computer vision}, publisher={Cambridge University Press}, author={Hartley, Richard and Zisserman, Andrew}, year={2004}} 
 
@article{T.M.COVER2012,
author = {{T.M. COVER}, P.E. HART},
pages = {1--28},
title = {{Nearest Neighbor Pattern Classfication}},
volume = {I},
year = {2012}
}

@article{Ullman1979,
author = {Ullman, S.},
file = {:C$\backslash$:/Users/IGNITER/Documents/442{\_}Longuet-Higgins1980.pdf:pdf},
journal = {Proc. R. Soc. London},
number = {1153},
pages = {405--426},
title = {{The Interpretation of Structure from Motion Author ( s ): S . Ullman Source : Proceedings of the Royal Society of London . Series B , Biological Sciences , Vol . 203 , Published by : Royal Society Stable URL : http://www.jstor.org/stable/77505}},
volume = {203},
year = {1979}
}
